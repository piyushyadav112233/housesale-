# -*- coding: utf-8 -*-
"""house-prices-advanced-regression-techniques.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-lbj0SQzkmPt6lmNNYa9xshSkyT9Xor4
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import pandas as pd
import zipfile

# Specify the zip file path
zip_file_path = '/content/house-prices-advanced-regression-techniques.zip'

# Specify the CSV file within the zip you want to read
csv_file_name = 'train.csv'  # Change to 'test.csv' or other if needed

# Open the zip file
with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
    # Extract the CSV file to memory
    with zip_ref.open(csv_file_name) as csv_file:
        # Read the CSV file using pandas
        df = pd.read_csv(csv_file)

# Now you can work with the dataframe 'df'
print(df.head())

df.info()

df.describe()

mean_sale_price = df['SalePrice'].mean()
median_sale_price = df['SalePrice'].median()
std_sale_price = df['SalePrice'].std()
min_sale_price = df['SalePrice'].min()
max_sale_price = df['SalePrice'].max()
print("Mean Sale Price:", mean_sale_price, "\nMedian Sale Price:", median_sale_price, "\nStandard Deviation of Sale Price:", std_sale_price, "\nMinimum Sale Price:", min_sale_price, "\nMaximum Sale Price:", max_sale_price)

# Measures of Dispersion:
range_sale_price = max_sale_price - min_sale_price
print("Range of Sale Price:", range_sale_price)
print("Variance of Sale Price:", df['SalePrice'].var())
print("Coefficient of Variation (CV) of Sale Price:", (std_sale_price / mean_sale_price) * 100)

#Interquartile Range (IQR): The difference between the 75th percentile (Q3) and the 25th percentile (Q1).
q1_sale_price = df['SalePrice'].quantile(0.25)
q3_sale_price = df['SalePrice'].quantile(0.75)
iqr_sale_price = q3_sale_price - q1_sale_price
print("Interquartile Range (IQR) of Sale Price:", iqr_sale_price)

#Skewness and Kurtosis:
skewness_sale_price = df['SalePrice'].skew()
kurtosis_sale_price = df['SalePrice'].kurtosis()
print("Skewness of Sale Price:", skewness_sale_price)
print("Kurtosis of Sale Price:", kurtosis_sale_price)

#Correlation:
correlation_matrix = df.corr(numeric_only=True)
print("Correlation Matrix:")
print(correlation_matrix)

df.isnull().sum()

#Descriptive Statistics Part I
df.describe()

# Convert data into a pandas DataFrame for easy manipulation
df = pd.DataFrame(df)
df

mean=df['SalePrice'].mean()
median=df['SalePrice'].median()
mode=df['SalePrice'].mode()
std=df['SalePrice'].std()
var=df['SalePrice'].var()
print("Mean:",mean)
print("Median:",median)
print("Mode:",mode)
print("Standard Deviation:",std)
print("Variance:",var)

df.duplicated().sum()

df.tail()

df.shape

#Calculate Measures of Dispersion
range_sale_price = df['SalePrice'].max() - df['SalePrice'].min()
print("Range of Sale Price:", range_sale_price)

range_lot_area = df['LotArea'].max() - df['LotArea'].min()
print("Range of Lot Area:", range_lot_area)

range_overall_qual = df['OverallQual'].max() - df['OverallQual'].min()
print("Range of Overall Quality:", range_overall_qual)

range_year_built = df['YearBuilt'].max() - df['YearBuilt'].min()
print("Range of Year Built:", range_year_built)

range_year_remod_add = df['YearRemodAdd'].max() - df['YearRemodAdd'].min()
print("Range of Year Remodel:", range_year_remod_add)

range_total_bsmt_sf = df['TotalBsmtSF'].max() - df['TotalBsmtSF'].min()
print("Range of Total Basement Area:", range_total_bsmt_sf)

range_1st_flr_sf = df['1stFlrSF'].max() - df['1stFlrSF'].min()
print("Range of First Floor Area:", range_1st_flr_sf)

range_2nd_flr_sf = df['2ndFlrSF'].max() - df['2ndFlrSF'].min()
print("Range of Second Floor Area:", range_2nd_flr_sf)

range_gr_liv_area = df['GrLivArea'].max() - df['GrLivArea'].min()
print("Range of Above-Grade Living Area:", range_gr_liv_area)

# Calculate Skewness and Kurtosis
skewness_sale_price = df['SalePrice'].skew()
kurtosis_sale_price = df['SalePrice'].kurtosis()
print("Skewness of Sale Price:", skewness_sale_price)
print("Kurtosis of Sale Price:", kurtosis_sale_price)

#Mean: Average value of the dataset.
#Median: Middle value when the data is sorted.
#Mode: Most frequent value in the dataset.
#Range: Difference between the maximum and minimum values.
#Variance: Measure of how much the data points differ from the mean.
#tandard Deviation: Square root of variance, showing how spread out the data is.

#Quartiles
q1_sale_price = df['SalePrice'].quantile(0.25)
q2_sale_price = df['SalePrice'].quantile(0.50)
q3_sale_price = df['SalePrice'].quantile(0.75)
print("First Quartile (Q1) of Sale Price:", q1_sale_price)
print("Second Quartile (Q2) of Sale Price:", q2_sale_price)
print("Third Quartile (Q3) of Sale Price:", q3_sale_price)

#Interquartile Range (IQR)
iqr_sale_price = q3_sale_price - q1_sale_price
print("Interquartile Range (IQR) of Sale Price:", iqr_sale_price)

#First Quartile (Q1): The median of the lower half of the data (25th percentile).
#Second Quartile (Q2): The median of the entire dataset (50th percentile).
#Third Quartile (Q3): The median of the upper half of the data (75th percentile).
#Interquartile Range (IQR): The difference between Q3 and Q1, which measures the spread of the middle 50% of the data.

#Descriptive Statistics Part 2
#1. Skewness:Skewness indicates whether a data distribution is symmetrical. A skewness value close to 0 suggests a symmetric distribution:
#Positive skew: The right tail is longer (right-skewed).
#Negative skew: The left tail is longer (left-skewed).

skew=df['SalePrice'].skew()
print("Skewness:",skew)

kurtosis_sale_price=df['SalePrice'].kurtosis()
print("Kurtosis:",kurtosis_sale_price)

#boxplot
sns.boxplot(df['SalePrice'])

sns.boxplot(df['LotArea'])

sns.boxplot(df['OverallQual'])

sns.boxplot(df['YearBuilt'])

plt.figure(figsize=(15,10))
sns.boxplot(df['YearRemodAdd'])

fig, axs = plt.subplots(2, 2, figsize=(15, 10))
sns.boxplot(df['TotalBsmtSF'], ax=axs[0, 0])
sns.boxplot(df['1stFlrSF'], ax=axs[0, 1])
sns.boxplot(df['2ndFlrSF'], ax=axs[1, 0])
sns.boxplot(df['GrLivArea'], ax=axs[1, 1])
plt.tight_layout()

fig, axs = plt.subplots(2, 2, figsize=(15, 10))
sns.boxplot(df['FullBath'], ax=axs[0, 0])
sns.boxplot(df['HalfBath'], ax=axs[0, 1])
sns.boxplot(df['BedroomAbvGr'], ax=axs[1, 0])
sns.boxplot(df['KitchenAbvGr'], ax=axs[1, 1])
plt.tight_layout()

fig, axs = plt.subplots(2, 2, figsize=(15, 10))
sns.boxplot(df['TotRmsAbvGrd'], ax=axs[0, 0])
sns.boxplot(df['Fireplaces'], ax=axs[0, 1])
sns.boxplot(df['GarageCars'], ax=axs[1, 0])
sns.boxplot(df['GarageArea'], ax=axs[1, 1])
plt.tight_layout()

#Effects of Linear Transformations:
## Define a linear transformation function
def linear_transform(x, a, b):
    return a * x + b

# Apply the transformation to 'SalePrice'
a = 2  # Scaling factor
b = 10000  # Shift factor
df['TransformedSalePrice'] = df['SalePrice'].apply(lambda x: linear_transform(x, a, b))
df[['SalePrice', 'TransformedSalePrice']].head()

plt.figure(figsize=(15,10))
sns.boxplot(df['TransformedSalePrice'])

sns.histplot(df['TransformedSalePrice'])

sns.histplot(df['SalePrice'])

sns.histplot(df['LotArea'])

sns.histplot(df['OverallQual'])

#zscore
from scipy.stats import zscore
df['zscore_SalePrice'] = zscore(df['SalePrice'])
df

df['zscore_SalePrice'].describe()

df['zscore_SalePrice'].skew()

df['zscore_SalePrice'].kurtosis()

df['zscore_SalePrice'].plot(kind='box')

df['zscore_SalePrice'].skew()

#Chebyshev's Rule
threshold=3
print(df['zscore_SalePrice'][np.abs(df['zscore_SalePrice'])>threshold])

mean=df['zscore_SalePrice'].mean()
std=df['zscore_SalePrice'].std()
print(mean,std)

#Define k (number of standard deviations):
k = 3

# Calculate the lower and upper bounds for outliers
lower_bound = mean - k * std
upper_bound = mean + k * std
print(lower_bound,upper_bound)

# Count the number of data points within the range
count_within_range = ((df['zscore_SalePrice'] >= lower_bound) & (df['zscore_SalePrice'] <= upper_bound)).sum()
count_within_range

# Print the results
print(f"Number of data points within {k} standard deviations of the mean: {count_within_range}")
print(f"Number of data points outside the range: {len(df['zscore_SalePrice']) - count_within_range}")
print(f"Percentage of data points outside the range: {(len(df['zscore_SalePrice']) - count_within_range) / len(df['zscore_SalePrice']) * 100:.2f}%")

# Plot the data and the bounds
plt.figure(figsize=(10, 6))
plt.hist(df['zscore_SalePrice'], bins=30, edgecolor='k')
plt.axvline(x=lower_bound, color='r', linestyle='--', label='Lower Bound')
plt.axvline(x=upper_bound, color='r', linestyle='--', label='Upper Bound')
plt.xlabel('zscore_SalePrice')
plt.ylabel('Frequency')
plt.title('Distribution of zscore_SalePrice with Bounds')
plt.legend()
plt.show()

# Pearson's Correlation Coefficient
correlation_matrix = df.corr(numeric_only=True)
correlation_matrix

correlation_matrix['SalePrice'].sort_values(ascending=False, key=abs, na_position='last')

correlation_matrix['SalePrice']=correlation_matrix['SalePrice'].sort_values(ascending=False, key=abs, na_position='last')
correlation_matrix

#plot
plt.figure(figsize=(15,10), dpi=80, facecolor='w', edgecolor='k', frameon=True)
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.show()

## Spearman's Rank Correlation
numeric_df = df.select_dtypes(include=np.number)
numeric_df

spearman_corr = numeric_df.corr(method='spearman')
spearman_corr

# Chi-Square Test of Independence (for categorical variables)
# Creating a contingency table for categorical data
contingency_table = pd.crosstab(df['OverallQual'], df['SalePrice'])
contingency_table

# Chi-Square Test
from scipy.stats import chi2_contingency
chi2, p, dof, expected = chi2_contingency(contingency_table)
print("Chi-Square Statistic:", chi2)
print("P-value:", p)
print("Degrees of Freedom:", dof)
print("Expected Frequencies:\n", expected)
print("P-value:", p)
print("Degrees of Freedom:", dof)
print("Expected Frequencies:\n", expected)

# Point-Biserial Correlation (continuous vs binary)
from scipy.stats import pointbiserialr
corr, p_value = pointbiserialr(df['SalePrice'], df['OverallQual'])
print("Point-Biserial Correlation Coefficient:", corr)
print("P-value:", p_value)

# Visualization: Scatter plot for Pearson's Correlation
plt.figure(figsize=(10, 6))
plt.scatter(df['SalePrice'], df['OverallQual'])
plt.xlabel('SalePrice')
plt.ylabel('OverallQual')
plt.title('Scatter Plot of SalePrice vs OverallQual')
plt.grid(True)
plt.show()

from scipy.stats import binom

#Binomial Distribution
#Define the parameters:
n = 100  # Number of trials
p = 0.4  # Probability of success in each trial
k = 2
Probability=binom.pmf(k,n,p)
print(Probability)

# Probability of getting exactly 6 successes (heads)
k = 6
probability = binom.pmf(k, n, p)
print("Probability of getting exactly 6 successes (heads):", probability)

#Uniform Distribution
from scipy.stats import uniform

# Plot a histogram to visualize the distribution
plt.figure(figsize=(10, 6))
plt.hist(df['SalePrice'], bins=30, density=True, alpha=0.6, color='b')
plt.xlabel('SalePrice')
plt.ylabel('Frequency')
plt.title('Distribution of SalePrice')
plt.grid(True)
plt.show()

#Normal Distribution
from scipy.stats import norm
normal_dis=norm.rvs(size=1000,loc=0,scale=1)
normal_dis

# Create a histogram to visualize the distribution
plt.figure(figsize=(10, 6))
plt.hist(normal_dis, bins=30, density=True, alpha=0.6, color='g')
plt.xlabel('SalePrice')
plt.ylabel('Frequency')
plt.title('Distribution of SalePrice')
plt.grid(True)
plt.show()

#Central Limit Theorem
sample_means=[]
for _ in range(100):
  sample=df['SalePrice'].sample(n=50,replace=True)
  sample_mean=sample.mean()
  sample_means.append(sample_mean)
  plt.hist(sample_means, bins=30, density=True, alpha=0.6, color='r')
  plt.xlabel('SalePrice')
  plt.ylabel('Frequency')
  plt.title('Distribution of SalePrice')
  plt.grid(True)
  plt.show()

# Cumulative probability
from scipy.stats import norm
Cumulative=norm.cdf(x=0,loc=0,scale=1)
Cumulative

# Expected number of successes (mean)
expected_mean = n * p
print("Expected number of successes (mean):", expected_mean)

# Expected number of successes (std)
expected_std = np.sqrt(n * p * (1 - p))
print("Expected number of successes (std):", expected_std)

import scipy.stats as stats
import numpy as np

# Calculate the sample mean and standard deviation
sample_mean = df['SalePrice'].mean()
sample_std = df['SalePrice'].std()

# Define the confidence level (e.g., 95%)
confidence_level = 0.95

# Calculate the margin of error
margin_of_error = stats.t.ppf((1 + confidence_level) / 2, df=len(df['SalePrice']) - 1) * sample_std / np.sqrt(len(df['SalePrice']))

# Calculate the confidence interval
confidence_interval = (sample_mean - margin_of_error, sample_mean + margin_of_error)

print("Confidence Interval:", confidence_interval)

#plot
plt.figure
plt.hist(df['SalePrice'], bins=30, density=True, alpha=0.6, color='b')
plt.xlabel('SalePrice')
plt.ylabel('Frequency')
plt.title('Distribution of SalePrice')
plt.grid(True)
plt.show()

z_score=np.abs(stats.zscore(df['SalePrice']))
z_score

# Standard Error for Proportion
standard_error = np.sqrt((p * (1 - p)) / n)
standard_error

# Margin of Error
margin_of_error = z_score * standard_error
margin_of_error

# Confidence Interval
confidence_interval = (p - margin_of_error, p + margin_of_error)
confidence_interval

print("Confidence Interval:", confidence_interval)

t_statistic, p_value = stats.ttest_ind(df['SalePrice'], df['OverallQual'])
print("T-statistic:", t_statistic)
print("P-value:", p_value)

# Set significance level (alpha)
alpha = 0.05

# Compare p-value
if p_value < alpha:
    print("Reject the null hypothesis: There is a significant difference in SalePrice and OverallQual.")
else:
    print("Fail to reject the null hypothesis: There is no significant difference in SalePrice and OverallQual.")

#encoding
from sklearn.preprocessing import LabelEncoder
encoder=LabelEncoder()
df['OverallQual']=encoder.fit_transform(df['OverallQual'])
df

#Model evalution
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import OneHotEncoder

#standardscaler
scaler=StandardScaler()
df['SalePrice']=scaler.fit_transform(df[['SalePrice']])
df

#model
X=df.drop('SalePrice',axis=1)
y=df['SalePrice']
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)
X_train

# Create a OneHotEncoder object
ohe = OneHotEncoder(sparse_output=False, drop='first') # Use sparse_output instead of sparse, and change the variable name
ohe

# Assuming X_train contains categorical features, select them
categorical_features = X_train.select_dtypes(include=['object']).columns
categorical_features

# Fit the encoder on the categorical features and transform them
encoded_features = ohe.fit_transform(X_train[categorical_features])
encoded_features

# Create a DataFrame from the encoded features
encoded_df = pd.DataFrame(encoded_features, columns=ohe.get_feature_names_out(categorical_features))
encoded_df

# Concatenate the encoded features with the numerical features
X_train_encoded = pd.concat([X_train.drop(columns=categorical_features), encoded_df], axis=1)
X_train_encoded

# Now, apply PCA to the encoded data
from sklearn.decomposition import PCA
from sklearn.impute import SimpleImputer
imputer=SimpleImputer(strategy='mean')
X_train_encoded_imputed=imputer.fit_transform(X_train_encoded)
pca=PCA(n_components=10)
X_train_pca=pca.fit_transform(X_train_encoded_imputed)
X_train_pca

# For X_test, impute missing values using the same imputer
ohe = OneHotEncoder(sparse_output=False, drop='first', handle_unknown='ignore')
ohe

all_categories = X_train[categorical_features].apply(pd.unique).apply(list).to_dict()
all_categories[categorical_features[14]].append('Membran')
ohe = OneHotEncoder(sparse_output=False, drop='first', categories=list(all_categories.values()))
print(ohe)

#k-means
from sklearn.cluster import KMeans
kmeans=KMeans(n_clusters=3,random_state=42)
kmeans.fit(X_train_pca)

# Create a KMeans object with 3 clusters
kmeans = KMeans(n_clusters=3, random_state=42)

# Fit the KMeans model to the data
kmeans.fit(X_train_pca)

# Get the cluster labels for each data point
cluster_labels = kmeans.labels_
print(cluster_labels)

#plot
plt.scatter(X_train_pca[:, 0], X_train_pca[:, 1], c=cluster_labels, cmap='viridis')
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.title('K-Means Clustering')
plt.show()

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

X = df[['OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF']]  # Select relevant features
y = df['SalePrice']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)
rf_regressor.fit(X_train, y_train)

#Make predictions
y_pred = rf_regressor.predict(X_test)
y_pred

#Evaluate the model:
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print("Mean Squared Error:", mse)
print("R-squared:", r2)

plt.figure(figsize=(8, 6))  # Adjust figure size if needed
plt.scatter(y_test, y_pred, alpha=0.5)  # Create scatter plot
plt.xlabel("Actual Values")
plt.ylabel("Predicted Values")
plt.title("Actual vs. Predicted Values (Random Forest Regression)")
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--')  # Add a diagonal line
plt.grid(True)
plt.show()

#Scatter Plot Matrix
from pandas.plotting import scatter_matrix # Change import statement to use pandas.plotting
names=['OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF']
scatter_matrix(df[names],figsize=(10,10))
plt.show()

#Classification Accuracy
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)  # Root Mean Squared Error
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("Mean Squared Error:", mse)
print("Root Mean Squared Error:", rmse)
print("Mean Absolute Error:", mae)
print("R-squared:", r2)

from sklearn.metrics import log_loss
from sklearn.preprocessing import LabelBinarizer

# Assuming y_test is the scaled target variable
y_test_original = y_test  # If y_test is already scaled, you might need to reverse the scaling here

# Discretize the target variable
y_test_discretized = pd.qcut(y_test_original, q=5, labels=False)

# Convert y_pred to probabilities using a classifier or other suitable method
# Here's an example using LabelBinarizer, but you might need a different approach
# depending on how you want to define your classes:

# Initialize LabelBinarizer
lb = LabelBinarizer()

# Fit LabelBinarizer on discretized target variable
lb.fit(y_test_discretized)

# Transform y_pred to probabilities
y_pred_probs = lb.transform(y_pred.astype(int))  # Convert y_pred to integers for binarization

# Calculate log loss
logloss = log_loss(y_test_discretized, y_pred_probs, labels=np.unique(y_test_discretized))
print(logloss)

logloss = log_loss(y_test_discretized, y_pred_probs, labels=np.unique(y_test_discretized))
print(logloss)

#Linear  Regression
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Instead of Logistic Regression, use a regression algorithm like Linear Regression:
from sklearn.linear_model import LinearRegression  # Import LinearRegression

regressor = LinearRegression()  # Create a LinearRegression object
regressor.fit(X_train, y_train)  # Fit the model
y_pred = regressor.predict(X_test)  # Make predictions

# Evaluate the model using appropriate metrics for regression:
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print("Mean Squared Error:", mse)
print("R-squared:", r2)

# Instead of Logistic Regression, use a regression algorithm like Linear Regression:
from sklearn.linear_model import LinearRegression  # Import LinearRegression

regressor = LinearRegression()  # Create a LinearRegression object
regressor.fit(X_train, y_train)  # Fit the model
y_pred = regressor.predict(X_test)  # Make predictions

# Evaluate the model using appropriate metrics for regression:
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print("Mean Squared Error:", mse)
print("R-squared:", r2)

from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

y_discretized = pd.qcut(y, q=5, labels=False)

# Split data with the discretized target variable
X_train, X_test, y_train_discretized, y_test_discretized = train_test_split(
    X, y_discretized, test_size=0.2, random_state=42
)

naive_bayes = GaussianNB()
naive_bayes.fit(X_train, y_train_discretized)

# Make predictions on the test set
y_pred = naive_bayes.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test_discretized, y_pred)
print("Accuracy:", accuracy)

# Classification Report
print("Classification Report:\n", classification_report(y_test_discretized, y_pred))

#Classification and Regression Trees
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, mean_squared_error, r2_score

x=df[['OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF']]
y=df['SalePrice']
X_train,X_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)

model = DecisionTreeClassifier(random_state=42)

model = DecisionTreeRegressor(random_state=42)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
rmse=np.sqrt(mse)

print("Mean Squared Error:", mse)
print("R-squared:", r2)
print("Root Mean Squared Error:", rmse)
print("Mean Absolute Error:", mae)

#Data Preparation and Modeling Pipeline
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

# Create an imputer to fill missing values with the mean
   imputer = SimpleImputer(strategy='mean')

   # Fit the imputer to your training data and transform it
   X_train_imputed = imputer.fit_transform(X_train)
   X_test_imputed = imputer.transform(X_test)

from sklearn.preprocessing import OneHotEncoder

# Create a OneHotEncoder
encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')

# Fit the encoder to your training data and transform it
# Assuming 'categorical_column_name' is a placeholder for the actual categorical column
X_train_encoded = encoder.fit_transform(X_train[['OverallQual']])  # Replace 'categorical_column_name'
X_test_encoded = encoder.transform(X_test[['OverallQual']])    # Replace 'categorical_column_name'

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_imputed)
X_test_scaled = scaler.transform(X_test_imputed)

#Create a Pipeline:
pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy='mean')),
    ('encoder', OneHotEncoder(sparse_output=False, handle_unknown='ignore')),
    ('scaler', StandardScaler()),
    ('model', LinearRegression())
])

pipeline.fit(X_train, y_train)

y_pred = pipeline.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print("Mean Squared Error:", mse)
print("R-squared:", r2)

# Save Model Using Pickle
import pickle
filename = 'finalized_model.sav'
pickle.dump(model, open(filename, 'wb'))

#Finalize Your Model with Joblib
import joblib
filename = 'finalized_model.sav'
joblib.dump(model, filename)

loaded_model = joblib.load(filename)
result = loaded_model.score(X_test, y_test)
print(result)

!pip install streamlit==1.28.0

import streamlit as st
import re
st.title('House Price Prediction')
st.write('Enter the following details to predict the house price:')
range_overall_qual=st.slider('OverallQual',min_value=1,max_value=10,value=5)
gr_liv_area=st.number_input('GrLivArea')
garage_cars=st.number_input('GarageCars')
total_bsmt_sf=st.number_input('TotalBsmtSF')

#create d Dataframe for prediction
data={'OverallQual':[range_overall_qual],'GrLivArea':[gr_liv_area],'GarageCars':[garage_cars],'TotalBsmtSF':[total_bsmt_sf]}
df_prediction=pd.DataFrame(data)
df_prediction

# Make prediction when the button is clicked
if st.button('Predict'):
    prediction = loaded_model.predict(df_prediction)
    st.write('Predicted House Price:', prediction[0])
    st.success('Prediction Successful!')

!pip install pyngrok==6.0.0

from pyngrok import ngrok

!ngrok authtoken <2pznxQjS4PJKgJtiRSXjdAPYbqQ_2sD75bxKiQTUn2o8CyqpJ>  # Replace <your_authtoken> with your actual authtoken

!pkill ngrok

!ngrok version

!pip install --upgrade pyngrok
!ngrok authtoken 2pznxQjS4PJKgJtiRSXjdAPYbqQ_2sD75bxKiQTUn2o8CyqpJ #Replace this with your actual auth token without brackets
!pkill ngrok #kill any existing process
from pyngrok import ngrok
public_url = ngrok.connect(port='8501', config_path=None, options={'--log': 'stdout'}) #Connect and start ngrok
print(public_url)





